{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>name</th>\n",
       "      <th>time</th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "      <th>pics</th>\n",
       "      <th>resp</th>\n",
       "      <th>gmap_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>104959831058524862747</td>\n",
       "      <td>Samuel Benson</td>\n",
       "      <td>1630018420109</td>\n",
       "      <td>4</td>\n",
       "      <td>A pretty seamless experience from top to botto...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0x874d9bf9d1db7c85:0xf1c3706a2b497a3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100925653611153628883</td>\n",
       "      <td>Emily Probst</td>\n",
       "      <td>1629223016683</td>\n",
       "      <td>5</td>\n",
       "      <td>Great food and great service! Being so close t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0x874d9bf9d1db7c85:0xf1c3706a2b497a3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100496729305505499660</td>\n",
       "      <td>Brianna Dungan</td>\n",
       "      <td>1629141770542</td>\n",
       "      <td>5</td>\n",
       "      <td>Pleasant location.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0x874d9bf9d1db7c85:0xf1c3706a2b497a3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>115619895645642399701</td>\n",
       "      <td>Aaron Tim</td>\n",
       "      <td>1630363338649</td>\n",
       "      <td>5</td>\n",
       "      <td>Finally a wingstop in Provo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0x874d9bf9d1db7c85:0xf1c3706a2b497a3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>115522360097670804488</td>\n",
       "      <td>Heather Ann Bruin</td>\n",
       "      <td>1629597692008</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0x874d9bf9d1db7c85:0xf1c3706a2b497a3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 user_id               name           time  rating  \\\n",
       "0  104959831058524862747      Samuel Benson  1630018420109       4   \n",
       "1  100925653611153628883       Emily Probst  1629223016683       5   \n",
       "2  100496729305505499660     Brianna Dungan  1629141770542       5   \n",
       "3  115619895645642399701          Aaron Tim  1630363338649       5   \n",
       "4  115522360097670804488  Heather Ann Bruin  1629597692008       5   \n",
       "\n",
       "                                                text pics resp  \\\n",
       "0  A pretty seamless experience from top to botto...  NaN  NaN   \n",
       "1  Great food and great service! Being so close t...  NaN  NaN   \n",
       "2                                 Pleasant location.  NaN  NaN   \n",
       "3                        Finally a wingstop in Provo  NaN  NaN   \n",
       "4                                                NaN  NaN  NaN   \n",
       "\n",
       "                                gmap_id  \n",
       "0  0x874d9bf9d1db7c85:0xf1c3706a2b497a3  \n",
       "1  0x874d9bf9d1db7c85:0xf1c3706a2b497a3  \n",
       "2  0x874d9bf9d1db7c85:0xf1c3706a2b497a3  \n",
       "3  0x874d9bf9d1db7c85:0xf1c3706a2b497a3  \n",
       "4  0x874d9bf9d1db7c85:0xf1c3706a2b497a3  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rest_google_reviews = pd.read_csv(\"../Datasets/Restaurantes_Google_Reviews.csv\",delimiter=\",\")\n",
    "rest_google_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA disponible: True\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA disponible:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifica si CUDA est√° disponible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga modelos a la GPU a excepcion del modelo de tokens\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"finiteautomata/beto-sentiment-analysis\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"finiteautomata/beto-sentiment-analysis\")\n",
    "model = model.to(device)\n",
    "pipe_beto = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer, device=0 if str(device) == \"cuda\" else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Guille\\anaconda3\\envs\\ProyectML\\lib\\site-packages\\transformers\\pipelines\\base.py:1090: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesados: 7560/152797, % completado: 4.95%, Tiempo estimado restante: 0.32 horas\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mh:\\Repositorios\\repositorio-henry\\google-yelp08\\ML\\Analisis_sentimiento\\Sentiment_Cuda_Procesing.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/h%3A/Repositorios/repositorio-henry/google-yelp08/ML/Analisis_sentimiento/Sentiment_Cuda_Procesing.ipynb#W5sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# Itera sobre los reviews y muestra el progreso cada 1 minuto\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/h%3A/Repositorios/repositorio-henry/google-yelp08/ML/Analisis_sentimiento/Sentiment_Cuda_Procesing.ipynb#W5sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mfor\u001b[39;00m index, row \u001b[39min\u001b[39;00m rest_google_reviews\u001b[39m.\u001b[39miterrows():\n\u001b[1;32m---> <a href='vscode-notebook-cell:/h%3A/Repositorios/repositorio-henry/google-yelp08/ML/Analisis_sentimiento/Sentiment_Cuda_Procesing.ipynb#W5sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     rest_google_reviews\u001b[39m.\u001b[39mat[index, \u001b[39m'\u001b[39m\u001b[39msentiment\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m sentiment_status(row[\u001b[39m'\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/h%3A/Repositorios/repositorio-henry/google-yelp08/ML/Analisis_sentimiento/Sentiment_Cuda_Procesing.ipynb#W5sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     elementos_procesados \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/h%3A/Repositorios/repositorio-henry/google-yelp08/ML/Analisis_sentimiento/Sentiment_Cuda_Procesing.ipynb#W5sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39m# Calcula el tiempo transcurrido\u001b[39;00m\n",
      "\u001b[1;32mh:\\Repositorios\\repositorio-henry\\google-yelp08\\ML\\Analisis_sentimiento\\Sentiment_Cuda_Procesing.ipynb Cell 6\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/h%3A/Repositorios/repositorio-henry/google-yelp08/ML/Analisis_sentimiento/Sentiment_Cuda_Procesing.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m      <a href='vscode-notebook-cell:/h%3A/Repositorios/repositorio-henry/google-yelp08/ML/Analisis_sentimiento/Sentiment_Cuda_Procesing.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     text \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(review)[:\u001b[39m512\u001b[39m]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/h%3A/Repositorios/repositorio-henry/google-yelp08/ML/Analisis_sentimiento/Sentiment_Cuda_Procesing.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     clasificacion \u001b[39m=\u001b[39m pipe_beto(text)\n\u001b[0;32m      <a href='vscode-notebook-cell:/h%3A/Repositorios/repositorio-henry/google-yelp08/ML/Analisis_sentimiento/Sentiment_Cuda_Procesing.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m clasificacion[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Guille\\anaconda3\\envs\\ProyectML\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:156\u001b[0m, in \u001b[0;36mTextClassificationPipeline.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    123\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[39m    Classify the text(s) given as inputs.\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[39m        If `top_k` is used, one such dictionary is returned per label.\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 156\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    157\u001b[0m     \u001b[39m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     _legacy \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtop_k\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m kwargs\n",
      "File \u001b[1;32mc:\\Users\\Guille\\anaconda3\\envs\\ProyectML\\lib\\site-packages\\transformers\\pipelines\\base.py:1129\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1121\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(\n\u001b[0;32m   1122\u001b[0m         \u001b[39miter\u001b[39m(\n\u001b[0;32m   1123\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1126\u001b[0m         )\n\u001b[0;32m   1127\u001b[0m     )\n\u001b[0;32m   1128\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1129\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[1;32mc:\\Users\\Guille\\anaconda3\\envs\\ProyectML\\lib\\site-packages\\transformers\\pipelines\\base.py:1136\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1134\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_single\u001b[39m(\u001b[39mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m   1135\u001b[0m     model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpreprocess_params)\n\u001b[1;32m-> 1136\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[0;32m   1137\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocess(model_outputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpostprocess_params)\n\u001b[0;32m   1138\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\Guille\\anaconda3\\envs\\ProyectML\\lib\\site-packages\\transformers\\pipelines\\base.py:1035\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1033\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[0;32m   1034\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m-> 1035\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[0;32m   1036\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m   1037\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Guille\\anaconda3\\envs\\ProyectML\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:187\u001b[0m, in \u001b[0;36mTextClassificationPipeline._forward\u001b[1;34m(self, model_inputs)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39muse_cache\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m inspect\u001b[39m.\u001b[39msignature(model_forward)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    186\u001b[0m     model_inputs[\u001b[39m\"\u001b[39m\u001b[39muse_cache\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m--> 187\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs)\n",
      "File \u001b[1;32mc:\\Users\\Guille\\anaconda3\\envs\\ProyectML\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Guille\\anaconda3\\envs\\ProyectML\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1562\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1554\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1555\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1556\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1557\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1560\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1562\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(\n\u001b[0;32m   1563\u001b[0m     input_ids,\n\u001b[0;32m   1564\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1565\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m   1566\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m   1567\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1568\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1569\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1570\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1571\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1572\u001b[0m )\n\u001b[0;32m   1574\u001b[0m pooled_output \u001b[39m=\u001b[39m outputs[\u001b[39m1\u001b[39m]\n\u001b[0;32m   1576\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[1;32mc:\\Users\\Guille\\anaconda3\\envs\\ProyectML\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Guille\\anaconda3\\envs\\ProyectML\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1022\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1013\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1015\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[0;32m   1016\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m   1017\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1020\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1021\u001b[0m )\n\u001b[1;32m-> 1022\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m   1023\u001b[0m     embedding_output,\n\u001b[0;32m   1024\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m   1025\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1026\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   1027\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m   1028\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1029\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1030\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1031\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1032\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1033\u001b[0m )\n\u001b[0;32m   1034\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1035\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Guille\\anaconda3\\envs\\ProyectML\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Guille\\anaconda3\\envs\\ProyectML\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:612\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    603\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    604\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    605\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    609\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    610\u001b[0m     )\n\u001b[0;32m    611\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 612\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    613\u001b[0m         hidden_states,\n\u001b[0;32m    614\u001b[0m         attention_mask,\n\u001b[0;32m    615\u001b[0m         layer_head_mask,\n\u001b[0;32m    616\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    617\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    618\u001b[0m         past_key_value,\n\u001b[0;32m    619\u001b[0m         output_attentions,\n\u001b[0;32m    620\u001b[0m     )\n\u001b[0;32m    622\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    623\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\Guille\\anaconda3\\envs\\ProyectML\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Guille\\anaconda3\\envs\\ProyectML\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:497\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    486\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    487\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    494\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m    495\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    496\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[0;32m    498\u001b[0m         hidden_states,\n\u001b[0;32m    499\u001b[0m         attention_mask,\n\u001b[0;32m    500\u001b[0m         head_mask,\n\u001b[0;32m    501\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    502\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[0;32m    503\u001b[0m     )\n\u001b[0;32m    504\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    506\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Guille\\anaconda3\\envs\\ProyectML\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Guille\\anaconda3\\envs\\ProyectML\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:427\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    418\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    419\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    425\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    426\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m--> 427\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[0;32m    428\u001b[0m         hidden_states,\n\u001b[0;32m    429\u001b[0m         attention_mask,\n\u001b[0;32m    430\u001b[0m         head_mask,\n\u001b[0;32m    431\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    432\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    433\u001b[0m         past_key_value,\n\u001b[0;32m    434\u001b[0m         output_attentions,\n\u001b[0;32m    435\u001b[0m     )\n\u001b[0;32m    436\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[0;32m    437\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Guille\\anaconda3\\envs\\ProyectML\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Guille\\anaconda3\\envs\\ProyectML\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:367\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    363\u001b[0m     attention_probs \u001b[39m=\u001b[39m attention_probs \u001b[39m*\u001b[39m head_mask\n\u001b[0;32m    365\u001b[0m context_layer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(attention_probs, value_layer)\n\u001b[1;32m--> 367\u001b[0m context_layer \u001b[39m=\u001b[39m context_layer\u001b[39m.\u001b[39;49mpermute(\u001b[39m0\u001b[39;49m, \u001b[39m2\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39m3\u001b[39;49m)\u001b[39m.\u001b[39;49mcontiguous()\n\u001b[0;32m    368\u001b[0m new_context_layer_shape \u001b[39m=\u001b[39m context_layer\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m] \u001b[39m+\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_head_size,)\n\u001b[0;32m    369\u001b[0m context_layer \u001b[39m=\u001b[39m context_layer\u001b[39m.\u001b[39mview(new_context_layer_shape)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Funci√≥n para procesar la rese√±a y mostrar progreso cada 1 minuto\n",
    "def sentiment_status(review):\n",
    "    if review is None:\n",
    "        return None\n",
    "    else:\n",
    "        text = str(review)[:512]\n",
    "        clasificacion = pipe_beto(text)\n",
    "        return clasificacion[0]['label']\n",
    "\n",
    "# Contador de reviews procesados\n",
    "elementos_procesados = 0\n",
    "\n",
    "# Tiempo inicial\n",
    "tiempo_inicial = time.time()\n",
    "\n",
    "# Itera sobre los reviews y muestra el progreso cada 1 minuto\n",
    "for index, row in rest_google_reviews.iterrows():\n",
    "    rest_google_reviews.at[index, 'sentiment'] = sentiment_status(row['text'])\n",
    "    elementos_procesados += 1\n",
    "    \n",
    "    # Calcula el tiempo transcurrido\n",
    "    tiempo_transcurrido = time.time() - tiempo_inicial\n",
    "    \n",
    "    # Calcula el tiempo estimado restante\n",
    "    elementos_restantes = len(rest_google_reviews) - elementos_procesados\n",
    "    tiempo_restante = (tiempo_transcurrido / elementos_procesados) * elementos_restantes\n",
    "    \n",
    "    # Verifica si ha pasado 1 minuto\n",
    "    if tiempo_transcurrido >= 60:\n",
    "        tiempo_inicial = time.time()\n",
    "        print(f\"Procesados: {elementos_procesados}/{len(rest_google_reviews)}, % completado: {round(elementos_procesados/len(rest_google_reviews)*100, 2)}%, Tiempo estimado restante: {round((tiempo_restante/60)/60,2)} horas\")\n",
    "\n",
    "# Mensaje final\n",
    "print(\"Procesamiento completado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>user_id</th>\n",
       "      <th>name_x</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>104959831058524862747</td>\n",
       "      <td>Samuel Benson</td>\n",
       "      <td>A pretty seamless experience from top to botto...</td>\n",
       "      <td>NEU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>100925653611153628883</td>\n",
       "      <td>Emily Probst</td>\n",
       "      <td>Great food and great service! Being so close t...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>100496729305505499660</td>\n",
       "      <td>Brianna Dungan</td>\n",
       "      <td>Pleasant location.</td>\n",
       "      <td>NEU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>115619895645642399701</td>\n",
       "      <td>Aaron Tim</td>\n",
       "      <td>Finally a wingstop in Provo</td>\n",
       "      <td>NEU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>115522360097670804488</td>\n",
       "      <td>Heather Ann Bruin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NEU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>111139967532022596352</td>\n",
       "      <td>Hootie Hennessey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NEU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>104959831058524862747</td>\n",
       "      <td>Samuel Benson</td>\n",
       "      <td>A pretty seamless experience from top to botto...</td>\n",
       "      <td>NEU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>100925653611153628883</td>\n",
       "      <td>Emily Probst</td>\n",
       "      <td>Great food and great service! Being so close t...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>100496729305505499660</td>\n",
       "      <td>Brianna Dungan</td>\n",
       "      <td>Pleasant location.</td>\n",
       "      <td>NEU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>115619895645642399701</td>\n",
       "      <td>Aaron Tim</td>\n",
       "      <td>Finally a wingstop in Provo</td>\n",
       "      <td>NEU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>115522360097670804488</td>\n",
       "      <td>Heather Ann Bruin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NEU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>111139967532022596352</td>\n",
       "      <td>Hootie Hennessey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NEU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>116839886208413920152</td>\n",
       "      <td>Andrea Ayala</td>\n",
       "      <td>This establishment is very family oriented and...</td>\n",
       "      <td>NEU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>111930204658217490762</td>\n",
       "      <td>Jeanette Rodgers</td>\n",
       "      <td>Wow. Already has three 5-star reviews. Very re...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>110612871828582311715</td>\n",
       "      <td>Lissette Diaz</td>\n",
       "      <td>Best best best Mexican food. Authentic, colorf...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>108947133017703821995</td>\n",
       "      <td>Keumi</td>\n",
       "      <td>Absolutely delicious, best Mexican food in tow...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>107619552409361444485</td>\n",
       "      <td>Myron Stratton</td>\n",
       "      <td>The food tasted amazing and our little waitres...</td>\n",
       "      <td>NEU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>116839886208413920152</td>\n",
       "      <td>Andrea Ayala</td>\n",
       "      <td>This establishment is very family oriented and...</td>\n",
       "      <td>NEU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>111930204658217490762</td>\n",
       "      <td>Jeanette Rodgers</td>\n",
       "      <td>Wow. Already has three 5-star reviews. Very re...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>110612871828582311715</td>\n",
       "      <td>Lissette Diaz</td>\n",
       "      <td>Best best best Mexican food. Authentic, colorf...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                user_id             name_x  \\\n",
       "0            0  104959831058524862747      Samuel Benson   \n",
       "1            1  100925653611153628883       Emily Probst   \n",
       "2            2  100496729305505499660     Brianna Dungan   \n",
       "3            3  115619895645642399701          Aaron Tim   \n",
       "4            4  115522360097670804488  Heather Ann Bruin   \n",
       "5            5  111139967532022596352   Hootie Hennessey   \n",
       "6            6  104959831058524862747      Samuel Benson   \n",
       "7            7  100925653611153628883       Emily Probst   \n",
       "8            8  100496729305505499660     Brianna Dungan   \n",
       "9            9  115619895645642399701          Aaron Tim   \n",
       "10          10  115522360097670804488  Heather Ann Bruin   \n",
       "11          11  111139967532022596352   Hootie Hennessey   \n",
       "12          12  116839886208413920152       Andrea Ayala   \n",
       "13          13  111930204658217490762   Jeanette Rodgers   \n",
       "14          14  110612871828582311715      Lissette Diaz   \n",
       "15          15  108947133017703821995              Keumi   \n",
       "16          16  107619552409361444485     Myron Stratton   \n",
       "17          17  116839886208413920152       Andrea Ayala   \n",
       "18          18  111930204658217490762   Jeanette Rodgers   \n",
       "19          19  110612871828582311715      Lissette Diaz   \n",
       "\n",
       "                                                 text sentiment  \n",
       "0   A pretty seamless experience from top to botto...       NEU  \n",
       "1   Great food and great service! Being so close t...       POS  \n",
       "2                                  Pleasant location.       NEU  \n",
       "3                         Finally a wingstop in Provo       NEU  \n",
       "4                                                 NaN       NEU  \n",
       "5                                                 NaN       NEU  \n",
       "6   A pretty seamless experience from top to botto...       NEU  \n",
       "7   Great food and great service! Being so close t...       POS  \n",
       "8                                  Pleasant location.       NEU  \n",
       "9                         Finally a wingstop in Provo       NEU  \n",
       "10                                                NaN       NEU  \n",
       "11                                                NaN       NEU  \n",
       "12  This establishment is very family oriented and...       NEU  \n",
       "13  Wow. Already has three 5-star reviews. Very re...       POS  \n",
       "14  Best best best Mexican food. Authentic, colorf...       POS  \n",
       "15  Absolutely delicious, best Mexican food in tow...       POS  \n",
       "16  The food tasted amazing and our little waitres...       NEU  \n",
       "17  This establishment is very family oriented and...       NEU  \n",
       "18  Wow. Already has three 5-star reviews. Very re...       POS  \n",
       "19  Best best best Mexican food. Authentic, colorf...       POS  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rest_google_reviews.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el tokenizer y el modelo en el dispositivo\n",
    "tokenizer = AutoTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para procesar la rese√±a y mostrar progreso cada 1 minuto\n",
    "def sentiment_stars(review):\n",
    "    if review is None:\n",
    "        return None\n",
    "    else:\n",
    "        text = str(review)[:512]\n",
    "        tokens = tokenizer.encode(text, return_tensors='pt').to(device)\n",
    "        result = model(tokens)[0]  # Access the logits\n",
    "        return int(torch.argmax(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesados: 6388/152797, % completado: 4.18%, Tiempo estimado restante: 0.38 horas\n",
      "Procesados: 12881/152797, % completado: 8.43%, Tiempo estimado restante: 0.18 horas\n",
      "Procesados: 19306/152797, % completado: 12.64%, Tiempo estimado restante: 0.12 horas\n",
      "Procesados: 25747/152797, % completado: 16.85%, Tiempo estimado restante: 0.08 horas\n",
      "Procesados: 32279/152797, % completado: 21.13%, Tiempo estimado restante: 0.06 horas\n",
      "Procesados: 38877/152797, % completado: 25.44%, Tiempo estimado restante: 0.05 horas\n",
      "Procesados: 45492/152797, % completado: 29.77%, Tiempo estimado restante: 0.04 horas\n",
      "Procesados: 51947/152797, % completado: 34.0%, Tiempo estimado restante: 0.03 horas\n",
      "Procesados: 58451/152797, % completado: 38.25%, Tiempo estimado restante: 0.03 horas\n",
      "Procesados: 65011/152797, % completado: 42.55%, Tiempo estimado restante: 0.02 horas\n",
      "Procesados: 71558/152797, % completado: 46.83%, Tiempo estimado restante: 0.02 horas\n",
      "Procesados: 78113/152797, % completado: 51.12%, Tiempo estimado restante: 0.02 horas\n",
      "Procesados: 84680/152797, % completado: 55.42%, Tiempo estimado restante: 0.01 horas\n",
      "Procesados: 91321/152797, % completado: 59.77%, Tiempo estimado restante: 0.01 horas\n",
      "Procesados: 97865/152797, % completado: 64.05%, Tiempo estimado restante: 0.01 horas\n",
      "Procesados: 104454/152797, % completado: 68.36%, Tiempo estimado restante: 0.01 horas\n",
      "Procesados: 110512/152797, % completado: 72.33%, Tiempo estimado restante: 0.01 horas\n",
      "Procesados: 116630/152797, % completado: 76.33%, Tiempo estimado restante: 0.01 horas\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mh:\\Repositorios\\repositorio-henry\\google-yelp08\\ML\\Analisis_sentimiento\\Sentiment_Cuda_Procesing.ipynb Cell 12\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/h%3A/Repositorios/repositorio-henry/google-yelp08/ML/Analisis_sentimiento/Sentiment_Cuda_Procesing.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Itera sobre los reviews y muestra el progreso cada 1 minuto\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/h%3A/Repositorios/repositorio-henry/google-yelp08/ML/Analisis_sentimiento/Sentiment_Cuda_Procesing.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m index, row \u001b[39min\u001b[39;00m rest_google_reviews\u001b[39m.\u001b[39miterrows():\n\u001b[1;32m----> <a href='vscode-notebook-cell:/h%3A/Repositorios/repositorio-henry/google-yelp08/ML/Analisis_sentimiento/Sentiment_Cuda_Procesing.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     rest_google_reviews\u001b[39m.\u001b[39mat[index, \u001b[39m'\u001b[39m\u001b[39msentiment\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m sentiment_stars(row[\u001b[39m'\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/h%3A/Repositorios/repositorio-henry/google-yelp08/ML/Analisis_sentimiento/Sentiment_Cuda_Procesing.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     elementos_procesados \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/h%3A/Repositorios/repositorio-henry/google-yelp08/ML/Analisis_sentimiento/Sentiment_Cuda_Procesing.ipynb#X12sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m# Calcula el tiempo transcurrido\u001b[39;00m\n",
      "\u001b[1;32mh:\\Repositorios\\repositorio-henry\\google-yelp08\\ML\\Analisis_sentimiento\\Sentiment_Cuda_Procesing.ipynb Cell 12\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/h%3A/Repositorios/repositorio-henry/google-yelp08/ML/Analisis_sentimiento/Sentiment_Cuda_Procesing.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m text \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(review)[:\u001b[39m512\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/h%3A/Repositorios/repositorio-henry/google-yelp08/ML/Analisis_sentimiento/Sentiment_Cuda_Procesing.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m tokens \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mencode(text, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/h%3A/Repositorios/repositorio-henry/google-yelp08/ML/Analisis_sentimiento/Sentiment_Cuda_Procesing.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m result \u001b[39m=\u001b[39m model(tokens)[\u001b[39m0\u001b[39m]  \u001b[39m# Access the logits\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/h%3A/Repositorios/repositorio-henry/google-yelp08/ML/Analisis_sentimiento/Sentiment_Cuda_Procesing.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mint\u001b[39m(torch\u001b[39m.\u001b[39margmax(result))\n",
      "File \u001b[1;32mc:\\Users\\Guille\\anaconda3\\envs\\ProyectML\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Guille\\anaconda3\\envs\\ProyectML\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1562\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1554\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1555\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1556\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1557\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1560\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1562\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(\n\u001b[0;32m   1563\u001b[0m     input_ids,\n\u001b[0;32m   1564\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1565\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m   1566\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m   1567\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1568\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1569\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1570\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1571\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1572\u001b[0m )\n\u001b[0;32m   1574\u001b[0m pooled_output \u001b[39m=\u001b[39m outputs[\u001b[39m1\u001b[39m]\n\u001b[0;32m   1576\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[1;32mc:\\Users\\Guille\\anaconda3\\envs\\ProyectML\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Guille\\anaconda3\\envs\\ProyectML\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1022\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1013\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1015\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[0;32m   1016\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m   1017\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1020\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1021\u001b[0m )\n\u001b[1;32m-> 1022\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m   1023\u001b[0m     embedding_output,\n\u001b[0;32m   1024\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m   1025\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1026\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   1027\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m   1028\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1029\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1030\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1031\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1032\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1033\u001b[0m )\n\u001b[0;32m   1034\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1035\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Guille\\anaconda3\\envs\\ProyectML\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Guille\\anaconda3\\envs\\ProyectML\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:612\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    603\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    604\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    605\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    609\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    610\u001b[0m     )\n\u001b[0;32m    611\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 612\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    613\u001b[0m         hidden_states,\n\u001b[0;32m    614\u001b[0m         attention_mask,\n\u001b[0;32m    615\u001b[0m         layer_head_mask,\n\u001b[0;32m    616\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    617\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    618\u001b[0m         past_key_value,\n\u001b[0;32m    619\u001b[0m         output_attentions,\n\u001b[0;32m    620\u001b[0m     )\n\u001b[0;32m    622\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    623\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\Guille\\anaconda3\\envs\\ProyectML\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Guille\\anaconda3\\envs\\ProyectML\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:539\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    536\u001b[0m     cross_attn_present_key_value \u001b[39m=\u001b[39m cross_attention_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m    537\u001b[0m     present_key_value \u001b[39m=\u001b[39m present_key_value \u001b[39m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 539\u001b[0m layer_output \u001b[39m=\u001b[39m apply_chunking_to_forward(\n\u001b[0;32m    540\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeed_forward_chunk, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchunk_size_feed_forward, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseq_len_dim, attention_output\n\u001b[0;32m    541\u001b[0m )\n\u001b[0;32m    542\u001b[0m outputs \u001b[39m=\u001b[39m (layer_output,) \u001b[39m+\u001b[39m outputs\n\u001b[0;32m    544\u001b[0m \u001b[39m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Guille\\anaconda3\\envs\\ProyectML\\lib\\site-packages\\transformers\\pytorch_utils.py:239\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    236\u001b[0m     \u001b[39m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    237\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(output_chunks, dim\u001b[39m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 239\u001b[0m \u001b[39mreturn\u001b[39;00m forward_fn(\u001b[39m*\u001b[39;49minput_tensors)\n",
      "File \u001b[1;32mc:\\Users\\Guille\\anaconda3\\envs\\ProyectML\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:551\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfeed_forward_chunk\u001b[39m(\u001b[39mself\u001b[39m, attention_output):\n\u001b[1;32m--> 551\u001b[0m     intermediate_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mintermediate(attention_output)\n\u001b[0;32m    552\u001b[0m     layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[0;32m    553\u001b[0m     \u001b[39mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32mc:\\Users\\Guille\\anaconda3\\envs\\ProyectML\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Guille\\anaconda3\\envs\\ProyectML\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:451\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m--> 451\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdense(hidden_states)\n\u001b[0;32m    452\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[0;32m    453\u001b[0m     \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\Guille\\anaconda3\\envs\\ProyectML\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Guille\\anaconda3\\envs\\ProyectML\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Contador de reviews procesados\n",
    "elementos_procesados = 0\n",
    "\n",
    "# Tiempo inicial\n",
    "tiempo_inicial = time.time()\n",
    "\n",
    "# Itera sobre los reviews y muestra el progreso cada 1 minuto\n",
    "for index, row in rest_google_reviews.iterrows():\n",
    "    rest_google_reviews.at[index, 'sentiment'] = sentiment_stars(row['text'])\n",
    "    elementos_procesados += 1\n",
    "    \n",
    "    # Calcula el tiempo transcurrido\n",
    "    tiempo_transcurrido = time.time() - tiempo_inicial\n",
    "    \n",
    "    # Calcula el tiempo estimado restante\n",
    "    elementos_restantes = len(rest_google_reviews) - elementos_procesados\n",
    "    tiempo_restante = (tiempo_transcurrido / elementos_procesados) * elementos_restantes\n",
    "    \n",
    "    # Verifica si ha pasado 1 minuto\n",
    "    if tiempo_transcurrido >= 60:\n",
    "        tiempo_inicial = time.time()\n",
    "        print(f\"Procesados: {elementos_procesados}/{len(rest_google_reviews)}, % completado: {round(elementos_procesados/len(rest_google_reviews)*100, 2)}%, Tiempo estimado restante: {round((tiempo_restante/60)/60,2)} horas\")\n",
    "\n",
    "# Mensaje final\n",
    "print(\"Procesamiento completado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>name</th>\n",
       "      <th>time</th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "      <th>pics</th>\n",
       "      <th>resp</th>\n",
       "      <th>gmap_id</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>104959831058524862747</td>\n",
       "      <td>Samuel Benson</td>\n",
       "      <td>1630018420109</td>\n",
       "      <td>4</td>\n",
       "      <td>A pretty seamless experience from top to botto...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0x874d9bf9d1db7c85:0xf1c3706a2b497a3</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100925653611153628883</td>\n",
       "      <td>Emily Probst</td>\n",
       "      <td>1629223016683</td>\n",
       "      <td>5</td>\n",
       "      <td>Great food and great service! Being so close t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0x874d9bf9d1db7c85:0xf1c3706a2b497a3</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100496729305505499660</td>\n",
       "      <td>Brianna Dungan</td>\n",
       "      <td>1629141770542</td>\n",
       "      <td>5</td>\n",
       "      <td>Pleasant location.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0x874d9bf9d1db7c85:0xf1c3706a2b497a3</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>115619895645642399701</td>\n",
       "      <td>Aaron Tim</td>\n",
       "      <td>1630363338649</td>\n",
       "      <td>5</td>\n",
       "      <td>Finally a wingstop in Provo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0x874d9bf9d1db7c85:0xf1c3706a2b497a3</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>115522360097670804488</td>\n",
       "      <td>Heather Ann Bruin</td>\n",
       "      <td>1629597692008</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0x874d9bf9d1db7c85:0xf1c3706a2b497a3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>111139967532022596352</td>\n",
       "      <td>Hootie Hennessey</td>\n",
       "      <td>1629774057722</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0x874d9bf9d1db7c85:0xf1c3706a2b497a3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>116839886208413920152</td>\n",
       "      <td>Andrea Ayala</td>\n",
       "      <td>1630539640545</td>\n",
       "      <td>5</td>\n",
       "      <td>This establishment is very family oriented and...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'time': 1630944387641, 'text': 'Thank you, An...</td>\n",
       "      <td>0x87530f390e058629:0x292320115b1705ea</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>111930204658217490762</td>\n",
       "      <td>Jeanette Rodgers</td>\n",
       "      <td>1625534118216</td>\n",
       "      <td>4</td>\n",
       "      <td>Wow. Already has three 5-star reviews. Very re...</td>\n",
       "      <td>[{'url': ['https://lh5.googleusercontent.com/p...</td>\n",
       "      <td>{'time': 1630945915991, 'text': \"Jeanette, Tha...</td>\n",
       "      <td>0x87530f390e058629:0x292320115b1705ea</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>110612871828582311715</td>\n",
       "      <td>Lissette Diaz</td>\n",
       "      <td>1627706899742</td>\n",
       "      <td>5</td>\n",
       "      <td>Best best best Mexican food. Authentic, colorf...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'time': 1630946322270, 'text': \"Thank you, so...</td>\n",
       "      <td>0x87530f390e058629:0x292320115b1705ea</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>108947133017703821995</td>\n",
       "      <td>Keumi</td>\n",
       "      <td>1627766593988</td>\n",
       "      <td>5</td>\n",
       "      <td>Absolutely delicious, best Mexican food in tow...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'time': 1630946176765, 'text': \"Keumi, Thank ...</td>\n",
       "      <td>0x87530f390e058629:0x292320115b1705ea</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>107619552409361444485</td>\n",
       "      <td>Myron Stratton</td>\n",
       "      <td>1629037179505</td>\n",
       "      <td>5</td>\n",
       "      <td>The food tasted amazing and our little waitres...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'time': 1630945453884, 'text': 'Thank you, My...</td>\n",
       "      <td>0x87530f390e058629:0x292320115b1705ea</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>101730082392267222039</td>\n",
       "      <td>Grant</td>\n",
       "      <td>1627675986370</td>\n",
       "      <td>5</td>\n",
       "      <td>This place is a great little fast food joint. ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0x875289613c18358f:0x7aaf75f9f2b3aaf1</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>117627983205761079364</td>\n",
       "      <td>Randy Sartori</td>\n",
       "      <td>1630365663925</td>\n",
       "      <td>3</td>\n",
       "      <td>Nice neighborhood eatery. Similar to Arctic Ci...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0x875289613c18358f:0x7aaf75f9f2b3aaf1</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>110878597916705790484</td>\n",
       "      <td>Rob Haynes</td>\n",
       "      <td>1630870168464</td>\n",
       "      <td>4</td>\n",
       "      <td>The burgers are bland, but the service and mil...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0x875289613c18358f:0x7aaf75f9f2b3aaf1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>114044108013272837740</td>\n",
       "      <td>David Kraft</td>\n",
       "      <td>1624711191262</td>\n",
       "      <td>5</td>\n",
       "      <td>The place looks run down and the menu, with al...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0x875289613c18358f:0x7aaf75f9f2b3aaf1</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>109629233645162599919</td>\n",
       "      <td>Waylon Robb</td>\n",
       "      <td>1626737315921</td>\n",
       "      <td>5</td>\n",
       "      <td>Well in my opinion this little burger joint is...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0x875289613c18358f:0x7aaf75f9f2b3aaf1</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>110815549893945895142</td>\n",
       "      <td>Josh Crawley</td>\n",
       "      <td>1622670237368</td>\n",
       "      <td>5</td>\n",
       "      <td>Good price, better quality than any fast food ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0x875289613c18358f:0x7aaf75f9f2b3aaf1</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>108884601576096301660</td>\n",
       "      <td>Chava Lee</td>\n",
       "      <td>1622748786413</td>\n",
       "      <td>5</td>\n",
       "      <td>This place was quick service and the food was ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0x875289613c18358f:0x7aaf75f9f2b3aaf1</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>117892122904436820329</td>\n",
       "      <td>John Kurtz</td>\n",
       "      <td>1588626270833</td>\n",
       "      <td>5</td>\n",
       "      <td>This review is specific solely to their shakes...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0x875289613c18358f:0x7aaf75f9f2b3aaf1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>103473739613187584747</td>\n",
       "      <td>justin johnson</td>\n",
       "      <td>1599876005488</td>\n",
       "      <td>5</td>\n",
       "      <td>Fast service, great prices, tons of options. C...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0x875289613c18358f:0x7aaf75f9f2b3aaf1</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user_id               name           time  rating  \\\n",
       "0   104959831058524862747      Samuel Benson  1630018420109       4   \n",
       "1   100925653611153628883       Emily Probst  1629223016683       5   \n",
       "2   100496729305505499660     Brianna Dungan  1629141770542       5   \n",
       "3   115619895645642399701          Aaron Tim  1630363338649       5   \n",
       "4   115522360097670804488  Heather Ann Bruin  1629597692008       5   \n",
       "5   111139967532022596352   Hootie Hennessey  1629774057722       5   \n",
       "6   116839886208413920152       Andrea Ayala  1630539640545       5   \n",
       "7   111930204658217490762   Jeanette Rodgers  1625534118216       4   \n",
       "8   110612871828582311715      Lissette Diaz  1627706899742       5   \n",
       "9   108947133017703821995              Keumi  1627766593988       5   \n",
       "10  107619552409361444485     Myron Stratton  1629037179505       5   \n",
       "11  101730082392267222039              Grant  1627675986370       5   \n",
       "12  117627983205761079364      Randy Sartori  1630365663925       3   \n",
       "13  110878597916705790484         Rob Haynes  1630870168464       4   \n",
       "14  114044108013272837740        David Kraft  1624711191262       5   \n",
       "15  109629233645162599919        Waylon Robb  1626737315921       5   \n",
       "16  110815549893945895142       Josh Crawley  1622670237368       5   \n",
       "17  108884601576096301660          Chava Lee  1622748786413       5   \n",
       "18  117892122904436820329         John Kurtz  1588626270833       5   \n",
       "19  103473739613187584747     justin johnson  1599876005488       5   \n",
       "\n",
       "                                                 text  \\\n",
       "0   A pretty seamless experience from top to botto...   \n",
       "1   Great food and great service! Being so close t...   \n",
       "2                                  Pleasant location.   \n",
       "3                         Finally a wingstop in Provo   \n",
       "4                                                 NaN   \n",
       "5                                                 NaN   \n",
       "6   This establishment is very family oriented and...   \n",
       "7   Wow. Already has three 5-star reviews. Very re...   \n",
       "8   Best best best Mexican food. Authentic, colorf...   \n",
       "9   Absolutely delicious, best Mexican food in tow...   \n",
       "10  The food tasted amazing and our little waitres...   \n",
       "11  This place is a great little fast food joint. ...   \n",
       "12  Nice neighborhood eatery. Similar to Arctic Ci...   \n",
       "13  The burgers are bland, but the service and mil...   \n",
       "14  The place looks run down and the menu, with al...   \n",
       "15  Well in my opinion this little burger joint is...   \n",
       "16  Good price, better quality than any fast food ...   \n",
       "17  This place was quick service and the food was ...   \n",
       "18  This review is specific solely to their shakes...   \n",
       "19  Fast service, great prices, tons of options. C...   \n",
       "\n",
       "                                                 pics  \\\n",
       "0                                                 NaN   \n",
       "1                                                 NaN   \n",
       "2                                                 NaN   \n",
       "3                                                 NaN   \n",
       "4                                                 NaN   \n",
       "5                                                 NaN   \n",
       "6                                                 NaN   \n",
       "7   [{'url': ['https://lh5.googleusercontent.com/p...   \n",
       "8                                                 NaN   \n",
       "9                                                 NaN   \n",
       "10                                                NaN   \n",
       "11                                                NaN   \n",
       "12                                                NaN   \n",
       "13                                                NaN   \n",
       "14                                                NaN   \n",
       "15                                                NaN   \n",
       "16                                                NaN   \n",
       "17                                                NaN   \n",
       "18                                                NaN   \n",
       "19                                                NaN   \n",
       "\n",
       "                                                 resp  \\\n",
       "0                                                 NaN   \n",
       "1                                                 NaN   \n",
       "2                                                 NaN   \n",
       "3                                                 NaN   \n",
       "4                                                 NaN   \n",
       "5                                                 NaN   \n",
       "6   {'time': 1630944387641, 'text': 'Thank you, An...   \n",
       "7   {'time': 1630945915991, 'text': \"Jeanette, Tha...   \n",
       "8   {'time': 1630946322270, 'text': \"Thank you, so...   \n",
       "9   {'time': 1630946176765, 'text': \"Keumi, Thank ...   \n",
       "10  {'time': 1630945453884, 'text': 'Thank you, My...   \n",
       "11                                                NaN   \n",
       "12                                                NaN   \n",
       "13                                                NaN   \n",
       "14                                                NaN   \n",
       "15                                                NaN   \n",
       "16                                                NaN   \n",
       "17                                                NaN   \n",
       "18                                                NaN   \n",
       "19                                                NaN   \n",
       "\n",
       "                                  gmap_id  sentiment  \n",
       "0    0x874d9bf9d1db7c85:0xf1c3706a2b497a3        4.0  \n",
       "1    0x874d9bf9d1db7c85:0xf1c3706a2b497a3        4.0  \n",
       "2    0x874d9bf9d1db7c85:0xf1c3706a2b497a3        3.0  \n",
       "3    0x874d9bf9d1db7c85:0xf1c3706a2b497a3        4.0  \n",
       "4    0x874d9bf9d1db7c85:0xf1c3706a2b497a3        1.0  \n",
       "5    0x874d9bf9d1db7c85:0xf1c3706a2b497a3        1.0  \n",
       "6   0x87530f390e058629:0x292320115b1705ea        4.0  \n",
       "7   0x87530f390e058629:0x292320115b1705ea        2.0  \n",
       "8   0x87530f390e058629:0x292320115b1705ea        4.0  \n",
       "9   0x87530f390e058629:0x292320115b1705ea        4.0  \n",
       "10  0x87530f390e058629:0x292320115b1705ea        4.0  \n",
       "11  0x875289613c18358f:0x7aaf75f9f2b3aaf1        3.0  \n",
       "12  0x875289613c18358f:0x7aaf75f9f2b3aaf1        3.0  \n",
       "13  0x875289613c18358f:0x7aaf75f9f2b3aaf1        2.0  \n",
       "14  0x875289613c18358f:0x7aaf75f9f2b3aaf1        3.0  \n",
       "15  0x875289613c18358f:0x7aaf75f9f2b3aaf1        3.0  \n",
       "16  0x875289613c18358f:0x7aaf75f9f2b3aaf1        4.0  \n",
       "17  0x875289613c18358f:0x7aaf75f9f2b3aaf1        4.0  \n",
       "18  0x875289613c18358f:0x7aaf75f9f2b3aaf1        0.0  \n",
       "19  0x875289613c18358f:0x7aaf75f9f2b3aaf1        4.0  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rest_google_reviews.head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sentiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
